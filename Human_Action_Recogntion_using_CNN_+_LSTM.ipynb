{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ft-u2b1iUQpg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\ashis\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\ashis\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: youtube-dl in c:\\users\\ashis\\anaconda3\\lib\\site-packages (2021.12.17)\n",
      "Requirement already satisfied: moviepy in c:\\users\\ashis\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: pydot in c:\\users\\ashis\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.9)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.10)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from moviepy) (2.28.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from moviepy) (0.4.8)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from moviepy) (2.19.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (9.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.9.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries.\n",
    "!pip install tensorflow opencv-contrib-python youtube-dl moviepy pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pafy\n",
      "  Cloning https://github.com/TahaAnwar/pafy.git to c:\\users\\ashis\\appdata\\local\\temp\\pip-install-tatvl1tb\\pafy_5fdee0239a66417787838613542ed926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/TahaAnwar/pafy.git 'C:\\Users\\ashis\\AppData\\Local\\Temp\\pip-install-tatvl1tb\\pafy_5fdee0239a66417787838613542ed926'\n",
      "  fatal: unable to access 'https://github.com/TahaAnwar/pafy.git/': Failed to connect to github.com port 443 after 21077 ms: Timed out\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  git clone --filter=blob:none --quiet https://github.com/TahaAnwar/pafy.git 'C:\\Users\\ashis\\AppData\\Local\\Temp\\pip-install-tatvl1tb\\pafy_5fdee0239a66417787838613542ed926' did not run successfully.\n",
      "  exit code: 128\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "git clone --filter=blob:none --quiet https://github.com/TahaAnwar/pafy.git 'C:\\Users\\ashis\\AppData\\Local\\Temp\\pip-install-tatvl1tb\\pafy_5fdee0239a66417787838613542ed926' did not run successfully.\n",
      "exit code: 128\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/TahaAnwar/pafy.git#egg=pafy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "X3AdbpZFCRR0"
   },
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9AFIAq0Q0VwG"
   },
   "outputs": [],
   "source": [
    "seed_constant = 27\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gb4Fv7Ag-kwb"
   },
   "outputs": [],
   "source": [
    "# Specify the height and width to which each video frame will be resized in our dataset.\n",
    "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
    "\n",
    "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# Specify the directory containing the UCF50 dataset. \n",
    "DATASET_DIR = \"SPHAR-Dataset-1.0/videos\"\n",
    "\n",
    "# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\n",
    "CLASSES_LIST = [name for name in os.listdir('SPHAR-Dataset-1.0/videos') if os.path.isdir(os.path.join('SPHAR-Dataset-1.0/videos', name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carcrash',\n",
       " 'falling',\n",
       " 'hitting',\n",
       " 'igniting',\n",
       " 'kicking',\n",
       " 'luggage',\n",
       " 'murdering',\n",
       " 'panicking',\n",
       " 'running',\n",
       " 'sitting',\n",
       " 'stealing',\n",
       " 'vandalizing']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSES_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zBu224OG-szz"
   },
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "\n",
    "    # Declare a list to store video frames.\n",
    "    frames_list = []\n",
    "    \n",
    "    # Read the Video File using the VideoCapture object.\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Iterate through the Video Frames.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "\n",
    "        # Reading the frame from the video. \n",
    "        success, frame = video_reader.read() \n",
    "\n",
    "        # Check if Video frame is not successfully read then break the loop\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Resize the Frame to fixed height and width.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
    "        normalized_frame = resized_frame / 255\n",
    "        \n",
    "        # Append the normalized frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    "    \n",
    "    # Release the VideoCapture object. \n",
    "    video_reader.release()\n",
    "\n",
    "    # Return the frames list.\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j6MLp9PpDJ4-"
   },
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \n",
    "    # Declared Empty Lists to store the features, labels and video file path values.\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "    \n",
    "    # Iterating through all the classes mentioned in the classes list\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        \n",
    "        # Display the name of the class whose data is being extracted.\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        \n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "        \n",
    "        i=0\n",
    "        # Iterate through all the files present in the files list.\n",
    "        for file_name in files_list:\n",
    "            if(i>20):\n",
    "                break\n",
    "            i=i+1\n",
    "            \n",
    "#             print(f'Extracting Data of '+file_name)\n",
    "            \n",
    "            # Get the complete video path.\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "\n",
    "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
    "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "\n",
    "                # Append the data to their repective lists.\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "\n",
    "    # Converting the list to numpy arrays\n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)  \n",
    "    \n",
    "    # Return the frames, class index, and video file path.\n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jNu1QTX0G0q"
   },
   "source": [
    "Now we will utilize the function **`create_dataset()`** created above to extract the data of the selected classes and create the required dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckAYgSriJN_R",
    "outputId": "c4cd44ea-f745-43ed-8a6a-cdad724b5569",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data of Class: carcrash\n",
      "Extracting Data of Class: falling\n",
      "Extracting Data of Class: hitting\n",
      "Extracting Data of Class: igniting\n",
      "Extracting Data of Class: kicking\n",
      "Extracting Data of Class: luggage\n",
      "Extracting Data of Class: murdering\n",
      "Extracting Data of Class: panicking\n",
      "Extracting Data of Class: running\n",
      "Extracting Data of Class: sitting\n",
      "Extracting Data of Class: stealing\n",
      "Extracting Data of Class: vandalizing\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset.\n",
    "features, labels, video_files_paths = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "KfcpuV6_Bd_T"
   },
   "outputs": [],
   "source": [
    "# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\n",
    "one_hot_encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UhkQjq1JJSO2"
   },
   "outputs": [],
   "source": [
    "# Split the Data into Train ( 75% ) and Test Set ( 25% ).\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,\n",
    "                                                                            test_size = 0.2, shuffle = True,\n",
    "                                                                            random_state = seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "h8cDSpfXJXwx"
   },
   "outputs": [],
   "source": [
    "def create_convlstm_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,\n",
    "                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "    \n",
    "    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = \"channels_last\",\n",
    "                         recurrent_dropout=0.2, return_sequences=True))\n",
    "    \n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = \"softmax\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2D1CS2Z6Jkp5",
    "outputId": "9d7d2e7e-f950-4f5c-c88b-810dc7c92c33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 20, 62, 62, 4)     1024      \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 20, 31, 31, 4)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 20, 31, 31, 4)    0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, 20, 29, 29, 8)     3488      \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 20, 15, 15, 8)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 20, 15, 15, 8)    0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, 20, 13, 13, 14)    11144     \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 20, 7, 7, 14)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 20, 7, 7, 14)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 20, 5, 5, 16)      17344     \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 20, 3, 3, 16)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2880)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                34572     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,572\n",
      "Trainable params: 67,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Construct the required convlstm model.\n",
    "convlstm_model = create_convlstm_model()\n",
    "\n",
    "# Display the success message. \n",
    "print(\"Model Created Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hbSK_rKUK-xQ",
    "outputId": "012b72f4-185a-4a61-e1f5-b3180ee95486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Plot the structure of the contructed model.\n",
    "plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yxx_adaPr8Vf",
    "outputId": "e085819d-1200-4298-f142-072cf573ad40",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "38/38 [==============================] - 56s 1s/step - loss: 2.4832 - accuracy: 0.1007 - val_loss: 2.5368 - val_accuracy: 0.0263\n",
      "Epoch 2/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 2.4442 - accuracy: 0.1275 - val_loss: 2.4391 - val_accuracy: 0.1053\n",
      "Epoch 3/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 2.2586 - accuracy: 0.2013 - val_loss: 2.2473 - val_accuracy: 0.1316\n",
      "Epoch 4/55\n",
      "38/38 [==============================] - 46s 1s/step - loss: 1.9379 - accuracy: 0.2752 - val_loss: 1.8943 - val_accuracy: 0.3158\n",
      "Epoch 5/55\n",
      "38/38 [==============================] - 47s 1s/step - loss: 1.7174 - accuracy: 0.3960 - val_loss: 1.7678 - val_accuracy: 0.3421\n",
      "Epoch 6/55\n",
      "38/38 [==============================] - 55s 1s/step - loss: 1.5129 - accuracy: 0.4497 - val_loss: 1.6173 - val_accuracy: 0.3684\n",
      "Epoch 7/55\n",
      "38/38 [==============================] - 50s 1s/step - loss: 1.4126 - accuracy: 0.4497 - val_loss: 1.4901 - val_accuracy: 0.3158\n",
      "Epoch 8/55\n",
      "38/38 [==============================] - 52s 1s/step - loss: 1.1325 - accuracy: 0.5772 - val_loss: 1.4714 - val_accuracy: 0.5263\n",
      "Epoch 9/55\n",
      "38/38 [==============================] - 53s 1s/step - loss: 1.0193 - accuracy: 0.5772 - val_loss: 1.7507 - val_accuracy: 0.4474\n",
      "Epoch 10/55\n",
      "38/38 [==============================] - 46s 1s/step - loss: 0.9504 - accuracy: 0.6376 - val_loss: 1.5018 - val_accuracy: 0.4737\n",
      "Epoch 11/55\n",
      "38/38 [==============================] - 48s 1s/step - loss: 0.8393 - accuracy: 0.6913 - val_loss: 1.3131 - val_accuracy: 0.5263\n",
      "Epoch 12/55\n",
      "38/38 [==============================] - 51s 1s/step - loss: 0.6978 - accuracy: 0.7248 - val_loss: 1.4642 - val_accuracy: 0.5526\n",
      "Epoch 13/55\n",
      "38/38 [==============================] - 47s 1s/step - loss: 0.7071 - accuracy: 0.7450 - val_loss: 1.6806 - val_accuracy: 0.4737\n",
      "Epoch 14/55\n",
      "38/38 [==============================] - 45s 1s/step - loss: 0.6676 - accuracy: 0.7315 - val_loss: 1.5691 - val_accuracy: 0.5263\n",
      "Epoch 15/55\n",
      "38/38 [==============================] - 48s 1s/step - loss: 0.5846 - accuracy: 0.7584 - val_loss: 1.6777 - val_accuracy: 0.4474\n",
      "Epoch 16/55\n",
      "38/38 [==============================] - 45s 1s/step - loss: 0.5923 - accuracy: 0.7383 - val_loss: 1.7347 - val_accuracy: 0.5263\n",
      "Epoch 17/55\n",
      "38/38 [==============================] - 54s 1s/step - loss: 0.5702 - accuracy: 0.7584 - val_loss: 1.8379 - val_accuracy: 0.5000\n",
      "Epoch 18/55\n",
      "38/38 [==============================] - 47s 1s/step - loss: 0.6494 - accuracy: 0.7517 - val_loss: 1.6201 - val_accuracy: 0.5526\n",
      "Epoch 19/55\n",
      "38/38 [==============================] - 50s 1s/step - loss: 0.5646 - accuracy: 0.7584 - val_loss: 1.6820 - val_accuracy: 0.6053\n",
      "Epoch 20/55\n",
      "38/38 [==============================] - 50s 1s/step - loss: 0.5600 - accuracy: 0.7584 - val_loss: 1.6122 - val_accuracy: 0.5000\n",
      "Epoch 21/55\n",
      "38/38 [==============================] - 55s 1s/step - loss: 0.5635 - accuracy: 0.7517 - val_loss: 1.5343 - val_accuracy: 0.5263\n",
      "Epoch 22/55\n",
      "38/38 [==============================] - 54s 1s/step - loss: 0.5094 - accuracy: 0.7651 - val_loss: 1.7342 - val_accuracy: 0.4737\n",
      "Epoch 23/55\n",
      "38/38 [==============================] - 47s 1s/step - loss: 0.5252 - accuracy: 0.7718 - val_loss: 1.7218 - val_accuracy: 0.5263\n",
      "Epoch 24/55\n",
      "38/38 [==============================] - 56s 1s/step - loss: 0.4725 - accuracy: 0.8121 - val_loss: 1.9632 - val_accuracy: 0.5000\n",
      "Epoch 25/55\n",
      "38/38 [==============================] - 56s 1s/step - loss: 0.4844 - accuracy: 0.7785 - val_loss: 1.8235 - val_accuracy: 0.5789\n",
      "Epoch 26/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.4794 - accuracy: 0.7785 - val_loss: 1.7477 - val_accuracy: 0.4474\n",
      "Epoch 27/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.4761 - accuracy: 0.7919 - val_loss: 1.8720 - val_accuracy: 0.5263\n",
      "Epoch 28/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.4902 - accuracy: 0.7987 - val_loss: 1.8192 - val_accuracy: 0.4737\n",
      "Epoch 29/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.7513 - accuracy: 0.7248 - val_loss: 1.7266 - val_accuracy: 0.4737\n",
      "Epoch 30/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.6030 - accuracy: 0.7987 - val_loss: 1.9254 - val_accuracy: 0.4474\n",
      "Epoch 31/55\n",
      "38/38 [==============================] - 41s 1s/step - loss: 0.5295 - accuracy: 0.7919 - val_loss: 2.0114 - val_accuracy: 0.5263\n",
      "Epoch 32/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.5129 - accuracy: 0.7987 - val_loss: 1.8509 - val_accuracy: 0.5000\n",
      "Epoch 33/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.4939 - accuracy: 0.7718 - val_loss: 1.9917 - val_accuracy: 0.5000\n",
      "Epoch 34/55\n",
      "38/38 [==============================] - 45s 1s/step - loss: 0.4465 - accuracy: 0.7852 - val_loss: 2.1453 - val_accuracy: 0.4474\n",
      "Epoch 35/55\n",
      "38/38 [==============================] - 50s 1s/step - loss: 0.4506 - accuracy: 0.8188 - val_loss: 2.1178 - val_accuracy: 0.4474\n",
      "Epoch 36/55\n",
      "38/38 [==============================] - 48s 1s/step - loss: 0.4707 - accuracy: 0.7987 - val_loss: 1.9064 - val_accuracy: 0.5526\n",
      "Epoch 37/55\n",
      "38/38 [==============================] - 45s 1s/step - loss: 0.4040 - accuracy: 0.8255 - val_loss: 2.1336 - val_accuracy: 0.5000\n",
      "Epoch 38/55\n",
      "38/38 [==============================] - 45s 1s/step - loss: 0.4008 - accuracy: 0.8322 - val_loss: 2.2547 - val_accuracy: 0.4737\n",
      "Epoch 39/55\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.4203 - accuracy: 0.8054 - val_loss: 2.0465 - val_accuracy: 0.4737\n",
      "Epoch 40/55\n",
      "38/38 [==============================] - 45s 1s/step - loss: 0.4720 - accuracy: 0.7651 - val_loss: 1.9065 - val_accuracy: 0.5000\n",
      "Epoch 41/55\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.4458 - accuracy: 0.8054 - val_loss: 1.9409 - val_accuracy: 0.4474\n",
      "Epoch 42/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.4313 - accuracy: 0.7987 - val_loss: 1.9607 - val_accuracy: 0.5000\n",
      "Epoch 43/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.4059 - accuracy: 0.8322 - val_loss: 2.1037 - val_accuracy: 0.5000\n",
      "Epoch 44/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.4462 - accuracy: 0.8054 - val_loss: 1.9994 - val_accuracy: 0.4737\n",
      "Epoch 45/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.4137 - accuracy: 0.8255 - val_loss: 2.0673 - val_accuracy: 0.5263\n",
      "Epoch 46/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.3902 - accuracy: 0.8188 - val_loss: 2.0494 - val_accuracy: 0.5789\n",
      "Epoch 47/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.3827 - accuracy: 0.8523 - val_loss: 2.1435 - val_accuracy: 0.4737\n",
      "Epoch 48/55\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.4259 - accuracy: 0.8188 - val_loss: 2.1548 - val_accuracy: 0.5263\n",
      "Epoch 49/55\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.4374 - accuracy: 0.8121 - val_loss: 2.1721 - val_accuracy: 0.5000\n",
      "Epoch 50/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.3986 - accuracy: 0.8322 - val_loss: 2.2569 - val_accuracy: 0.5000\n",
      "Epoch 51/55\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.4317 - accuracy: 0.8322 - val_loss: 2.2738 - val_accuracy: 0.5000\n",
      "Epoch 52/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.3713 - accuracy: 0.8255 - val_loss: 2.3898 - val_accuracy: 0.5000\n",
      "Epoch 53/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.4064 - accuracy: 0.8255 - val_loss: 2.2582 - val_accuracy: 0.5526\n",
      "Epoch 54/55\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.3467 - accuracy: 0.8322 - val_loss: 2.1225 - val_accuracy: 0.4474\n",
      "Epoch 55/55\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.4477 - accuracy: 0.8255 - val_loss: 2.6283 - val_accuracy: 0.3421\n"
     ]
    }
   ],
   "source": [
    "# Compile the model and specify loss function, optimizer and metrics values to the model\n",
    "convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n",
    "\n",
    "# Start training the model.\n",
    "convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 55, batch_size = 4,\n",
    "                                                     shuffle = True, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74tGjokkmSHR",
    "outputId": "f4a69c15-dab9-481b-85a2-fe8c1b3460c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 598ms/step - loss: 3.3539 - accuracy: 0.4681\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model.\n",
    "model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "GLfnP_czYudN"
   },
   "outputs": [],
   "source": [
    "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
    "\n",
    "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
    "current_date_time_dt = dt.datetime.now()\n",
    "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
    "\n",
    "model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
    "\n",
    "convlstm_model.save(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, video_files_paths\n",
    "\n",
    "features = np.array(features)\n",
    "np.save('features.npy',features)\n",
    "\n",
    "labels = np.array(labels)\n",
    "np.save('labels.npy',labels)\n",
    "\n",
    "with open('video_files_paths.txt', 'w') as f:\n",
    "    for item in video_files_paths:\n",
    "        f.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Human Action Recogntion using CNN + LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
